{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green\"> Numerical Simulation Laboratory </span>\n",
    "## <span style=\"color:brown\"> Python Exercise 12 </span>\n",
    "## <span style=\"color:orange\"> Keras - Deep & Convolutional Neural Network image recognition </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST with Keras\n",
    "\n",
    "The goal of exercise 12 is to use deep neural network models, implemented in the Keras python package, to recognize and distinguish between the ten handwritten digits (0-9).\n",
    "\n",
    "The MNIST dataset comprises $70000$ handwritten digits, each of which comes in a square image, divided into a $28\\times 28$ pixel grid. Every pixel can take on $256$ gradation of the gray color, interpolating between white and black, and hence each data point assumes any value in the set $\\{0,1,\\dots,255\\}$. Since there are $10$ categories in the problem, corresponding to the ten digits, this problem represents a generic **classification task**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "seed=0\n",
    "np.random.seed(seed) # fix random seed\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Load and Process the Data\n",
    "\n",
    "Keras can conveniently download the MNIST data from the web. All we need to do is import the `mnist` module and use the `load_data()` class, and it will create the training and test data sets or us.\n",
    "\n",
    "The MNIST set has pre-defined test and training sets, in order to facilitate the comparison of the performance of different models on the data.\n",
    "\n",
    "Once we have loaded the data, we need to format it in the correct shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60000, 28, 28)\n",
      "Y_train shape: (60000,)\n",
      "\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28 # number of pixels \n",
    "# output\n",
    "num_classes = 10 # 10 digits\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('Y_train shape:', Y_train.shape)\n",
    "print()\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an example of a data point with label 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an example of a data point with label 4 before to_categorical ...\n",
      "... and with label [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] after to_categorical\n",
      "\n",
      "X_train shape: (60000, 784)\n",
      "Y_train shape: (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "# reshape data, it could depend on Keras backend\n",
    "X_train = X_train.reshape(X_train.shape[0], img_rows*img_cols)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows*img_cols)\n",
    "\n",
    "# cast floats to single precesion\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# rescale data in interval [0,1]\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# look at an example of data point\n",
    "print('an example of a data point with label', Y_train[20])\n",
    "# matshow: display a matrix in a new figure window\n",
    "plt.matshow(X_train[20,:].reshape(28,28),cmap='binary')\n",
    "plt.show()\n",
    "\n",
    "print('an example of a data point with label', Y_train[20], 'before to_categorical ...')\n",
    "# convert class vectors to binary class matrices, e.g. for use with categorical_crossentropy\n",
    "Y_train = keras.utils.to_categorical(Y_train, num_classes)\n",
    "Y_test = keras.utils.to_categorical(Y_test, num_classes)\n",
    "print('... and with label', Y_train[20], 'after to_categorical')\n",
    "print()\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('Y_train shape:', Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Consider an array of 5 labels out of a set of 3 classes {0, 1, 2}:\n",
    "labels = np.array([0, 2, 1, 2, 0])\n",
    "# `to_categorical` converts this into a matrix with as many columns as there are classes.\n",
    "# The number of rows stays the same.\n",
    "keras.utils.to_categorical(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Define the Neural Net and its Architecture\n",
    "\n",
    "We can now move on to construct our deep neural net. We shall use Keras's `Sequential()` class to instantiate a model, and will add different deep layers one by one using the `add()` method\n",
    "\n",
    "For the purposes of our example, it suffices to focus on `Dense` layers for simplicity. Every `Dense()` layer accepts as its first required argument an integer which specifies the number of neurons. The type of activation function for the layer is defined using the `activation` optional argument, the input of which is the name of the activation function in `string` format. Examples include `relu`, `tanh`, `elu`, `sigmoid`, `softmax`. \n",
    "\n",
    "In order for our DNN to work properly, we have to make sure that the numbers of input and output neurons for each layer match. Therefore, we specify the shape of the input in the first layer of the model explicitly using the optional argument `input_shape=(N_features,)`. The sequential construction of the model then allows Keras to infer the correct input/output dimensions of all hidden layers automatically. Hence, we only need to specify the size of the softmax output layer to match the number of categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture created successfully!\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "def create_DNN():\n",
    "    # instantiate model\n",
    "    model = Sequential()\n",
    "    # add a dense all-to-all relu layer\n",
    "    model.add(Dense(400,input_shape=(img_rows*img_cols,), activation='relu'))\n",
    "    # add a dense all-to-all relu layer\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    # apply dropout with rate 0.5\n",
    "    model.add(Dropout(0.5))\n",
    "    # soft-max layer\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "print('Model architecture created successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Choose the Optimizer and the Cost Function\n",
    "\n",
    "Next, we choose the loss function according to which to train the DNN. For classification problems, this is the cross entropy, and since the output data was cast in categorical form, we choose the `categorical_crossentropy` defined in Keras' `losses` module. Depending on the problem of interest one can pick any other suitable loss function. To optimize the weights of the net, we choose SGD. This algorithm is already available to use under Keras' `optimizers` module, but we could use any other built-in one as well. The parameters for the optimizer, such as `lr` (learning rate) or `momentum` are passed using the corresponding optional arguments of the `SGD()` function. All available arguments can be found in Keras' online documentation at [https://keras.io/](https://keras.io/). While the loss function and the optimizer are essential for the training procedure, to test the performance of the model one may want to look at a particular `metric` of performance. For instance, in categorical tasks one typically looks at their `accuracy`, which is defined as the percentage of correctly classified data points. To complete the definition of our model, we use the `compile()` method, with optional arguments for the `optimizer`, `loss`, and the validation `metric` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model compiled successfully and ready to be trained.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam\n",
    "\n",
    "def compile_model():\n",
    "    # create the model\n",
    "    model=create_DNN()\n",
    "    # compile the model\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=SGD(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "print('Model compiled successfully and ready to be trained.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Train the model\n",
    "\n",
    "We train our DNN in minibatches. \n",
    "\n",
    "Shuffling the training data during training improves stability of the model. Thus, we train over a number of training epochs. Each epoch corresponds to using **all the train data** divided in minibatches.\n",
    "\n",
    "Training the DNN is a one-liner using the `fit()` method of the `Sequential` class. The first two required arguments are the training input and output data. As optional arguments, we specify the mini-`batch_size`, the number of training `epochs`, and the test or `validation_data`. To monitor the training procedure for every epoch, we set `verbose=True`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.7280 - accuracy: 0.7886 - val_loss: 0.3029 - val_accuracy: 0.9175\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3727 - accuracy: 0.8942 - val_loss: 0.2340 - val_accuracy: 0.9322\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3017 - accuracy: 0.9161 - val_loss: 0.1969 - val_accuracy: 0.9423\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2597 - accuracy: 0.9277 - val_loss: 0.1722 - val_accuracy: 0.9478\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2301 - accuracy: 0.9354 - val_loss: 0.1572 - val_accuracy: 0.9501\n"
     ]
    }
   ],
   "source": [
    "# training parameters\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "# create the deep neural net\n",
    "model_DNN = compile_model()\n",
    "\n",
    "# train DNN and store training info in history\n",
    "history = model_DNN.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Evaluate the Model Performance on the *Unseen* Test Data\n",
    "\n",
    "Next, we evaluate the model and read of the loss on the test data, and its accuracy using the `evaluate()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1572 - accuracy: 0.9501\n",
      "\n",
      "Test loss: 0.15722277760505676\n",
      "Test accuracy: 0.9501000046730042\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-aebe52a118e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# summarize history for accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'acc'"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "score = model_DNN.evaluate(X_test, Y_test, verbose=1)\n",
    "\n",
    "# print performance\n",
    "print()\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# look into training history\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.ylabel('model accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='best')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('model loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout, during training, slices off some artificial neurons, thus, training accuracy suffers.\n",
    "\n",
    "Dropout, during testing, turns itself off and allows all of the ‘weak classifiers’ in the neural network to be used. Thus, testing accuracy improves with respect to training accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_DNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-3f0d77fda512>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#X_test = X_test.reshape(X_test.shape[0], img_rows*img_cols)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_DNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_cols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_DNN' is not defined"
     ]
    }
   ],
   "source": [
    "#X_test = X_test.reshape(X_test.shape[0], img_rows*img_cols)\n",
    "predictions = model_DNN.predict(X_test)\n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols,1)\n",
    "\n",
    "plt.figure(figsize=(15, 15)) \n",
    "for i in range(10):    \n",
    "    ax = plt.subplot(2, 10, i + 1)    \n",
    "    plt.imshow(X_test[i, :, :, 0], cmap='gray')    \n",
    "    plt.title(\"Digit: {}\\nPredicted: {}\".format(np.argmax(Y_test[i]), np.argmax(predictions[i])))    \n",
    "    plt.axis('off') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12.1\n",
    "\n",
    "By keeping fixed all the other parameters, try to use at least two other optimizers, different from SGD. <span style=\"color:red\">Watch to accuracy and loss for training and validation data and comment on the performances</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Convolutional Neural Nets with Keras\n",
    "\n",
    "We have so far considered each MNIST data sample as a $(28\\times 28,)$-long 1d vector. On the other hand, we do know that in every one of the hand-written digits there are *local* spatial correlations between the pixels, but also *translational invariance*, which we would like to take advantage of to improve the accuracy of our classification model. To this end, we first need to reshape the training and test input data as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60000, 28, 28, 1)\n",
      "Y_train shape: (60000, 10)\n",
      "\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# you will need the following for Convolutional Neural Networks\n",
    "from keras.layers import Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "# reshape data, depending on Keras backend\n",
    "if keras.backend.image_data_format() == 'channels_first':\n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "    \n",
    "print('X_train shape:', X_train.shape)\n",
    "print('Y_train shape:', Y_train.shape)\n",
    "print()\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can ask the question of whether a neural net can learn to recognize such local patterns. This can be achieved by using convolutional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12.2\n",
    "\n",
    "Change the architecture of your DNN using convolutional layers. Use `Conv2D`, `MaxPooling2D`, `Dropout`, but also do not forget `Flatten`, a standard `Dense` layer and `soft-max` in the end. I have merged step 2 and 3 in the following definition of `create_CNN()` that **<span style=\"color:red\">you should complete</span>**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_CNN():\n",
    "    # instantiate model\n",
    "    model = Sequential()\n",
    "    # add first convolutional layer with 10 filters (dimensionality of output space)\n",
    "    model.add(Conv2D(10, kernel_size=(5, 5),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    #\n",
    "    # ADD HERE SOME OTHER LAYERS AT YOUR WILL, FOR EXAMPLE SOME: Dropout, 2D pooling, 2D convolutional etc. ... \n",
    "    # remember to move towards a standard flat layer in the final part of your DNN,\n",
    "    # and that we need a soft-max layer with num_classes=10 possible outputs\n",
    "    #\n",
    "    \n",
    "    # compile the model\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer='SGD',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your DCNN and evaluate its performance proceeding exactly as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "# create the deep conv net\n",
    "model_CNN=create_CNN()\n",
    "\n",
    "# train CNN\n",
    "model_CNN.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test))\n",
    "\n",
    "# evaliate model\n",
    "score = model_CNN.evaluate(X_test, Y_test, verbose=1)\n",
    "\n",
    "# print performance\n",
    "print()\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, **<span style=\"color:red\">add the evaluation of your CNN performances</span>** like that used for the DNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12.3\n",
    "\n",
    "Use the `gimp` application to create 10 pictures of your \"handwritten\" digits, import them in your jupyter-notebook and try to see if your CNN is able to recognize your handwritten digits.\n",
    "\n",
    "For example, you can use the following code to import a picture of an handwritten digit\n",
    "(Note: you should install Python Image Library (PIL/Pillow):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: 28x28\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAEyCAYAAACbGke8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "digit_filename = \"./Pictures/9b.png\"\n",
    "digit_in = Image.open(digit_filename).convert('L')\n",
    "\n",
    "ydim, xdim = digit_in.size\n",
    "print(\"Image size: \"+str(xdim)+\"x\"+str(ydim))\n",
    "pix=digit_in.load();\n",
    "data = np.zeros((xdim, ydim))\n",
    "for j in range(ydim):\n",
    "    for i in range(xdim):\n",
    "        data[i,j]=pix[j,i]\n",
    "\n",
    "data /= 255\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(data, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use the previously trained DNN to predict the digit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n",
      "(1, 784)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAE/CAYAAAAnhFRiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1ACkQqkBSIVSA5AKpQYgFUoNQCr/B1pQhPswWxqMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(data.shape)\n",
    "data = data.reshape(1,xdim*ydim)\n",
    "print(data.shape)\n",
    "pred_0 = model_DNN.predict(data)\n",
    "\n",
    "data = data.reshape(xdim,ydim)\n",
    "\n",
    "plt.figure(figsize=(5, 5))  \n",
    "plt.imshow(data, cmap='gray')    \n",
    "plt.title(\"Digit predicted:    {}\".format(np.argmax(pred_0)))\n",
    "plt.axis('off') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\">Supplementary information 1: Use of `gimp` </span>\n",
    "\n",
    "- from the Unix shell type: `gimp` and hit `Return`\n",
    "- File -> new (chose: 28x28 pixels)\n",
    "- rascale the image to 800%\n",
    "- Use the **brush** with dimension 2px to draw your digit\n",
    "- Color -> invert (to have black background)\n",
    "- Export the image as a `.png` file\n",
    "\n",
    "That's all!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\">Supplementary information 2: Display trained filters in your CNN</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print your NN layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_index=0\n",
    "for layer in model_CNN.layers:\n",
    "    print(layer_index, layer.name)\n",
    "    layer_index+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display your filters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer_index should be the index of a convolutional layer\n",
    "layer_index=0\n",
    "# retrieve weights from the convolutional hidden layer\n",
    "filters, biases = model_CNN.layers[layer_index].get_weights()\n",
    "# normalize filter values to 0-1 so we can visualize them\n",
    "f_min, f_max = filters.min(), filters.max()\n",
    "filters = (filters - f_min) / (f_max - f_min)\n",
    "print(filters.shape)\n",
    "\n",
    "# plot filters\n",
    "n_filters, ix = filters.shape[3], 1\n",
    "for i in range(n_filters):\n",
    "    # get the filter\n",
    "    f = filters[:, :, :, i]\n",
    "    # specify subplot and turn of axis\n",
    "    ax = plt.subplot(1,n_filters, ix)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    # plot filter channel in grayscale\n",
    "    plt.imshow(f[:, :, 0], cmap='gray')\n",
    "    ix += 1\n",
    "# show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\">Supplementary information 3: Monitor layer outputs in your CNN</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = 0\n",
    "\n",
    "from keras.models import Model\n",
    "layer_outputs = [layer.output for layer in model_CNN.layers]\n",
    "activation_model = Model(inputs=model_CNN.input, outputs=layer_outputs)\n",
    "activations = activation_model.predict(X_test[test_index].reshape(1,28,28,1))\n",
    " \n",
    "def display_activation(activations, col_size, row_size, layer_index): \n",
    "    activation = activations[layer_index]\n",
    "    activation_index=0\n",
    "    fig, ax = plt.subplots(row_size, col_size, figsize=(row_size*3,col_size*3))\n",
    "    for row in range(0,row_size):\n",
    "        for col in range(0,col_size):\n",
    "            ax[row][col].imshow(activation[0, :, :, activation_index], cmap='gray')\n",
    "            activation_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.imshow(X_test[test_index][:,:,0], cmap='gray')\n",
    "# def display_activation(activations, col_size, row_size, layer number)\n",
    "display_activation(activations, 4, 2, 0)\n",
    "# col_size x row_size must be <= Number of filters for the convolutional layer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
